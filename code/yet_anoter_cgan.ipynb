{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--randomseed'], dest='randomseed', nargs=None, const=None, default=42, type=<class 'int'>, choices=None, help='seed', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import os \n",
    "import argparse\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=False, help='cifar10 | lsun | mnist',default='mnist')\n",
    "parser.add_argument('--dataroot', required=False, help='path to data',default='../../affect-recognition/data/pics/')\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='image size input')\n",
    "parser.add_argument('--channels', type=int, default=1, help='number of channels')\n",
    "parser.add_argument('--latentdim', type=int, default=100, help='size of latent vector')\n",
    "parser.add_argument('--n_classes', type=int, default=10, help='number of classes in data set')\n",
    "parser.add_argument('--epoch', type=int, default=200, help='number of epoch')\n",
    "parser.add_argument('--lrate', type=float, default=0.0002, help='learning rate')\n",
    "parser.add_argument('--beta', type=float, default=0.5, help='beta for adam optimizer')\n",
    "parser.add_argument('--beta1', type=float, default=0.999, help='beta1 for adam optimizer')\n",
    "parser.add_argument('--output', default='../../affect-recognition/data/pics/', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--randomseed', type=int, help='seed',default=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt,u = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1/200][D loss: 0.446146][G loss: 1.147342]\n",
      "[Epoch: 2/200][D loss: 0.424998][G loss: 1.095250]\n",
      "[Epoch: 3/200][D loss: 0.538535][G loss: 1.661360]\n",
      "[Epoch: 4/200][D loss: 0.478863][G loss: 1.246031]\n",
      "[Epoch: 5/200][D loss: 0.450313][G loss: 1.370050]\n",
      "[Epoch: 6/200][D loss: 0.472542][G loss: 1.548119]\n",
      "[Epoch: 7/200][D loss: 0.569261][G loss: 1.175852]\n",
      "[Epoch: 8/200][D loss: 0.509154][G loss: 1.520882]\n",
      "[Epoch: 9/200][D loss: 0.620282][G loss: 0.997288]\n",
      "[Epoch: 10/200][D loss: 0.584178][G loss: 1.535263]\n",
      "[Epoch: 11/200][D loss: 0.556249][G loss: 1.195990]\n",
      "[Epoch: 12/200][D loss: 0.612358][G loss: 1.374866]\n",
      "[Epoch: 13/200][D loss: 0.640880][G loss: 1.130364]\n",
      "[Epoch: 14/200][D loss: 0.579006][G loss: 1.432647]\n",
      "[Epoch: 15/200][D loss: 0.576021][G loss: 0.794312]\n",
      "[Epoch: 16/200][D loss: 0.589840][G loss: 1.078212]\n",
      "[Epoch: 17/200][D loss: 0.530936][G loss: 1.193568]\n",
      "[Epoch: 18/200][D loss: 0.596794][G loss: 0.936076]\n",
      "[Epoch: 19/200][D loss: 0.593174][G loss: 0.858233]\n",
      "[Epoch: 20/200][D loss: 0.663647][G loss: 1.524854]\n",
      "[Epoch: 21/200][D loss: 0.584414][G loss: 1.108954]\n",
      "[Epoch: 22/200][D loss: 0.601987][G loss: 1.342934]\n",
      "[Epoch: 23/200][D loss: 0.577887][G loss: 1.112826]\n",
      "[Epoch: 24/200][D loss: 0.519858][G loss: 1.122035]\n",
      "[Epoch: 25/200][D loss: 0.667923][G loss: 1.637205]\n",
      "[Epoch: 26/200][D loss: 0.607251][G loss: 1.317232]\n",
      "[Epoch: 27/200][D loss: 0.536911][G loss: 0.879958]\n",
      "[Epoch: 28/200][D loss: 0.678813][G loss: 1.032704]\n",
      "[Epoch: 29/200][D loss: 0.645989][G loss: 0.718680]\n",
      "[Epoch: 30/200][D loss: 0.615063][G loss: 0.883742]\n",
      "[Epoch: 31/200][D loss: 1.022531][G loss: 0.398432]\n",
      "[Epoch: 32/200][D loss: 0.648207][G loss: 0.691974]\n",
      "[Epoch: 33/200][D loss: 0.627868][G loss: 0.736628]\n",
      "[Epoch: 34/200][D loss: 0.630527][G loss: 0.917549]\n",
      "[Epoch: 35/200][D loss: 0.582406][G loss: 1.179737]\n",
      "[Epoch: 36/200][D loss: 0.713443][G loss: 1.816163]\n",
      "[Epoch: 37/200][D loss: 0.613659][G loss: 1.228634]\n",
      "[Epoch: 38/200][D loss: 0.530248][G loss: 1.108286]\n",
      "[Epoch: 39/200][D loss: 0.593268][G loss: 0.984913]\n",
      "[Epoch: 40/200][D loss: 0.605460][G loss: 0.883158]\n",
      "[Epoch: 41/200][D loss: 0.536638][G loss: 0.947584]\n",
      "[Epoch: 42/200][D loss: 0.661052][G loss: 0.842163]\n",
      "[Epoch: 43/200][D loss: 0.681493][G loss: 0.626827]\n",
      "[Epoch: 44/200][D loss: 0.562406][G loss: 1.018618]\n",
      "[Epoch: 45/200][D loss: 0.607495][G loss: 1.522470]\n",
      "[Epoch: 46/200][D loss: 0.693195][G loss: 0.589588]\n",
      "[Epoch: 47/200][D loss: 0.697724][G loss: 0.674840]\n",
      "[Epoch: 48/200][D loss: 0.632808][G loss: 1.292188]\n",
      "[Epoch: 49/200][D loss: 0.642309][G loss: 0.835295]\n",
      "[Epoch: 50/200][D loss: 0.617080][G loss: 0.981414]\n",
      "[Epoch: 51/200][D loss: 0.675799][G loss: 0.857381]\n",
      "[Epoch: 52/200][D loss: 0.488677][G loss: 1.441768]\n",
      "[Epoch: 53/200][D loss: 0.559496][G loss: 1.903363]\n",
      "[Epoch: 54/200][D loss: 0.635155][G loss: 0.909006]\n",
      "[Epoch: 55/200][D loss: 0.619034][G loss: 0.858277]\n",
      "[Epoch: 56/200][D loss: 0.659545][G loss: 0.831889]\n",
      "[Epoch: 57/200][D loss: 0.542215][G loss: 1.321299]\n",
      "[Epoch: 58/200][D loss: 0.686661][G loss: 0.844165]\n",
      "[Epoch: 59/200][D loss: 0.828254][G loss: 0.470127]\n",
      "[Epoch: 60/200][D loss: 0.633509][G loss: 0.860854]\n",
      "[Epoch: 61/200][D loss: 0.770007][G loss: 0.498508]\n",
      "[Epoch: 62/200][D loss: 0.588886][G loss: 1.008774]\n",
      "[Epoch: 63/200][D loss: 0.534981][G loss: 1.346876]\n",
      "[Epoch: 64/200][D loss: 0.625094][G loss: 1.095560]\n",
      "[Epoch: 65/200][D loss: 0.610818][G loss: 0.973632]\n",
      "[Epoch: 66/200][D loss: 0.562585][G loss: 1.218229]\n",
      "[Epoch: 67/200][D loss: 0.606658][G loss: 2.090988]\n",
      "[Epoch: 68/200][D loss: 0.575537][G loss: 1.125415]\n",
      "[Epoch: 69/200][D loss: 0.570693][G loss: 2.177903]\n",
      "[Epoch: 70/200][D loss: 0.474831][G loss: 1.788928]\n",
      "[Epoch: 71/200][D loss: 0.560306][G loss: 1.396308]\n",
      "[Epoch: 72/200][D loss: 0.673426][G loss: 1.118787]\n",
      "[Epoch: 73/200][D loss: 0.637871][G loss: 1.098795]\n",
      "[Epoch: 74/200][D loss: 0.603456][G loss: 0.932158]\n",
      "[Epoch: 75/200][D loss: 0.625738][G loss: 1.014222]\n",
      "[Epoch: 76/200][D loss: 0.775245][G loss: 0.493463]\n",
      "[Epoch: 77/200][D loss: 0.533015][G loss: 1.100571]\n",
      "[Epoch: 78/200][D loss: 0.499468][G loss: 1.037815]\n",
      "[Epoch: 79/200][D loss: 0.524679][G loss: 2.540414]\n",
      "[Epoch: 80/200][D loss: 0.527369][G loss: 1.738611]\n",
      "[Epoch: 81/200][D loss: 0.582885][G loss: 0.984582]\n",
      "[Epoch: 82/200][D loss: 0.553052][G loss: 0.878749]\n",
      "[Epoch: 83/200][D loss: 0.641974][G loss: 0.990408]\n",
      "[Epoch: 84/200][D loss: 0.693638][G loss: 0.608395]\n",
      "[Epoch: 85/200][D loss: 0.548376][G loss: 1.430385]\n",
      "[Epoch: 86/200][D loss: 0.775898][G loss: 2.669739]\n",
      "[Epoch: 87/200][D loss: 0.511718][G loss: 1.571425]\n",
      "[Epoch: 88/200][D loss: 0.624570][G loss: 0.950201]\n",
      "[Epoch: 89/200][D loss: 0.547425][G loss: 1.796140]\n",
      "[Epoch: 90/200][D loss: 0.824610][G loss: 0.474832]\n",
      "[Epoch: 91/200][D loss: 0.534476][G loss: 2.243083]\n",
      "[Epoch: 92/200][D loss: 0.603187][G loss: 0.948863]\n",
      "[Epoch: 93/200][D loss: 0.624096][G loss: 0.852187]\n",
      "[Epoch: 94/200][D loss: 0.597201][G loss: 1.041407]\n",
      "[Epoch: 95/200][D loss: 0.605288][G loss: 0.864774]\n",
      "[Epoch: 96/200][D loss: 0.472497][G loss: 1.386150]\n",
      "[Epoch: 97/200][D loss: 0.658049][G loss: 0.911917]\n",
      "[Epoch: 98/200][D loss: 0.650133][G loss: 0.812242]\n",
      "[Epoch: 99/200][D loss: 0.726917][G loss: 0.555117]\n",
      "[Epoch: 100/200][D loss: 0.507660][G loss: 0.989579]\n",
      "[Epoch: 101/200][D loss: 0.628177][G loss: 0.979358]\n",
      "[Epoch: 102/200][D loss: 0.684772][G loss: 0.620376]\n",
      "[Epoch: 103/200][D loss: 0.540134][G loss: 1.194514]\n",
      "[Epoch: 104/200][D loss: 0.539282][G loss: 1.106345]\n",
      "[Epoch: 105/200][D loss: 0.537113][G loss: 1.380791]\n",
      "[Epoch: 106/200][D loss: 0.606819][G loss: 1.305585]\n",
      "[Epoch: 107/200][D loss: 0.584597][G loss: 1.007620]\n",
      "[Epoch: 108/200][D loss: 0.551871][G loss: 2.676967]\n",
      "[Epoch: 109/200][D loss: 0.558238][G loss: 1.356390]\n",
      "[Epoch: 110/200][D loss: 0.617938][G loss: 0.738180]\n",
      "[Epoch: 111/200][D loss: 0.608886][G loss: 0.922156]\n",
      "[Epoch: 112/200][D loss: 0.591585][G loss: 1.014331]\n",
      "[Epoch: 113/200][D loss: 0.457985][G loss: 1.807448]\n",
      "[Epoch: 114/200][D loss: 0.642730][G loss: 0.689927]\n",
      "[Epoch: 115/200][D loss: 0.525457][G loss: 1.349457]\n",
      "[Epoch: 116/200][D loss: 0.456890][G loss: 2.140795]\n",
      "[Epoch: 117/200][D loss: 0.456124][G loss: 1.775397]\n",
      "[Epoch: 118/200][D loss: 0.600642][G loss: 0.941405]\n",
      "[Epoch: 119/200][D loss: 0.572141][G loss: 0.911780]\n",
      "[Epoch: 120/200][D loss: 0.674707][G loss: 0.559721]\n",
      "[Epoch: 121/200][D loss: 0.542165][G loss: 1.719997]\n",
      "[Epoch: 122/200][D loss: 0.555065][G loss: 1.028826]\n",
      "[Epoch: 123/200][D loss: 0.582740][G loss: 0.880011]\n",
      "[Epoch: 124/200][D loss: 0.572750][G loss: 1.009704]\n",
      "[Epoch: 125/200][D loss: 0.524107][G loss: 1.200630]\n",
      "[Epoch: 126/200][D loss: 0.623211][G loss: 0.828892]\n",
      "[Epoch: 127/200][D loss: 0.452085][G loss: 1.887007]\n",
      "[Epoch: 128/200][D loss: 0.594750][G loss: 1.002036]\n",
      "[Epoch: 129/200][D loss: 0.474365][G loss: 2.136219]\n",
      "[Epoch: 130/200][D loss: 0.423184][G loss: 1.265119]\n",
      "[Epoch: 131/200][D loss: 0.485731][G loss: 1.674755]\n",
      "[Epoch: 132/200][D loss: 0.542463][G loss: 0.939891]\n",
      "[Epoch: 133/200][D loss: 0.759969][G loss: 0.512267]\n",
      "[Epoch: 134/200][D loss: 0.515900][G loss: 1.373929]\n",
      "[Epoch: 135/200][D loss: 0.520348][G loss: 1.878332]\n",
      "[Epoch: 136/200][D loss: 0.627529][G loss: 0.995175]\n",
      "[Epoch: 137/200][D loss: 0.460443][G loss: 2.124889]\n",
      "[Epoch: 138/200][D loss: 0.598409][G loss: 0.986108]\n",
      "[Epoch: 139/200][D loss: 0.612814][G loss: 0.968743]\n",
      "[Epoch: 140/200][D loss: 0.594826][G loss: 1.298571]\n",
      "[Epoch: 141/200][D loss: 0.492859][G loss: 1.305848]\n",
      "[Epoch: 142/200][D loss: 0.566745][G loss: 0.938697]\n",
      "[Epoch: 143/200][D loss: 0.560199][G loss: 1.068934]\n",
      "[Epoch: 144/200][D loss: 0.552547][G loss: 1.187563]\n",
      "[Epoch: 145/200][D loss: 0.381914][G loss: 1.605043]\n",
      "[Epoch: 146/200][D loss: 0.535716][G loss: 1.017329]\n",
      "[Epoch: 147/200][D loss: 0.418290][G loss: 1.553719]\n",
      "[Epoch: 148/200][D loss: 0.578915][G loss: 0.869264]\n",
      "[Epoch: 149/200][D loss: 0.917604][G loss: 0.465349]\n",
      "[Epoch: 150/200][D loss: 0.588733][G loss: 0.870185]\n",
      "[Epoch: 151/200][D loss: 0.479910][G loss: 1.521976]\n",
      "[Epoch: 152/200][D loss: 0.493391][G loss: 1.466349]\n",
      "[Epoch: 153/200][D loss: 0.752306][G loss: 0.514691]\n",
      "[Epoch: 154/200][D loss: 0.555657][G loss: 1.924616]\n",
      "[Epoch: 155/200][D loss: 0.537747][G loss: 0.982077]\n",
      "[Epoch: 156/200][D loss: 0.628181][G loss: 0.721302]\n",
      "[Epoch: 157/200][D loss: 0.467641][G loss: 1.931338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 158/200][D loss: 0.595762][G loss: 0.979018]\n",
      "[Epoch: 159/200][D loss: 0.425997][G loss: 2.507332]\n",
      "[Epoch: 160/200][D loss: 0.475792][G loss: 2.171040]\n",
      "[Epoch: 161/200][D loss: 0.554022][G loss: 0.931997]\n",
      "[Epoch: 162/200][D loss: 0.477525][G loss: 1.947245]\n",
      "[Epoch: 163/200][D loss: 0.496748][G loss: 1.199639]\n",
      "[Epoch: 164/200][D loss: 0.564644][G loss: 1.279712]\n",
      "[Epoch: 165/200][D loss: 0.544490][G loss: 1.424178]\n",
      "[Epoch: 166/200][D loss: 0.679059][G loss: 2.584683]\n",
      "[Epoch: 167/200][D loss: 0.332812][G loss: 3.166833]\n",
      "[Epoch: 168/200][D loss: 0.572656][G loss: 1.735618]\n",
      "[Epoch: 169/200][D loss: 0.525864][G loss: 1.282853]\n",
      "[Epoch: 170/200][D loss: 0.619113][G loss: 1.208614]\n",
      "[Epoch: 171/200][D loss: 0.604309][G loss: 0.871398]\n",
      "[Epoch: 172/200][D loss: 0.602969][G loss: 1.249989]\n",
      "[Epoch: 173/200][D loss: 0.550324][G loss: 1.163842]\n",
      "[Epoch: 174/200][D loss: 0.326341][G loss: 1.891591]\n",
      "[Epoch: 175/200][D loss: 0.501505][G loss: 1.319116]\n",
      "[Epoch: 176/200][D loss: 0.566315][G loss: 1.723239]\n",
      "[Epoch: 177/200][D loss: 0.494786][G loss: 1.578453]\n",
      "[Epoch: 178/200][D loss: 0.489445][G loss: 1.193782]\n",
      "[Epoch: 179/200][D loss: 0.569942][G loss: 1.411494]\n",
      "[Epoch: 180/200][D loss: 0.502089][G loss: 1.284697]\n",
      "[Epoch: 181/200][D loss: 0.511405][G loss: 1.494110]\n",
      "[Epoch: 182/200][D loss: 0.359476][G loss: 2.399636]\n",
      "[Epoch: 183/200][D loss: 0.525100][G loss: 1.931653]\n",
      "[Epoch: 184/200][D loss: 0.448678][G loss: 1.800393]\n",
      "[Epoch: 185/200][D loss: 0.472004][G loss: 1.565650]\n",
      "[Epoch: 186/200][D loss: 0.492140][G loss: 1.294764]\n",
      "[Epoch: 187/200][D loss: 0.474764][G loss: 1.575517]\n",
      "[Epoch: 188/200][D loss: 0.426991][G loss: 1.733449]\n",
      "[Epoch: 189/200][D loss: 0.453993][G loss: 2.590702]\n",
      "[Epoch: 190/200][D loss: 0.346489][G loss: 2.311784]\n",
      "[Epoch: 191/200][D loss: 0.390348][G loss: 2.636562]\n",
      "[Epoch: 192/200][D loss: 0.573415][G loss: 1.106937]\n",
      "[Epoch: 193/200][D loss: 0.332292][G loss: 2.329241]\n",
      "[Epoch: 194/200][D loss: 0.576280][G loss: 1.486649]\n",
      "[Epoch: 195/200][D loss: 0.428147][G loss: 1.931366]\n",
      "[Epoch: 196/200][D loss: 0.321240][G loss: 3.081236]\n",
      "[Epoch: 197/200][D loss: 0.360468][G loss: 2.096037]\n",
      "[Epoch: 198/200][D loss: 0.368320][G loss: 2.645573]\n",
      "[Epoch: 199/200][D loss: 0.465773][G loss: 1.824233]\n",
      "[Epoch: 200/200][D loss: 0.369809][G loss: 2.096931]\n"
     ]
    }
   ],
   "source": [
    "img_shape = (opt.channels, opt.imageSize, opt.imageSize)\n",
    "print(img_shape)\n",
    "cuda = True if torch.cuda.is_available() else False \n",
    "\n",
    "os.makedirs(opt.output, exist_ok=True)\n",
    "\n",
    "if opt.randomseed is None: \n",
    "    opt.randomseed = random.randint(1,10000)\n",
    "random.seed(opt.randomseed)\n",
    "torch.manual_seed(opt.randomseed)\n",
    "\n",
    "# preprocessing for mnist, lsun, cifar10\n",
    "if opt.dataset == 'mnist': \n",
    "    dataset = datasets.MNIST(root = opt.dataroot, train=True,download=True, \n",
    "        transform=transforms.Compose([transforms.Resize(opt.imageSize), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.5,), (0.5,))]))\n",
    "\n",
    "elif opt.dataset == 'lsun': \n",
    "    dataset = datasets.LSUN(root = opt.dataroot, train=True,download=True, \n",
    "        transform=transforms.Compose([transforms.Resize(opt.imageSize), \n",
    "            transforms.CenterCrop(opt.imageSize),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.5,), (0.5,))]))\n",
    "\n",
    "elif opt.dataset == 'cifar10':  \n",
    "    dataset = datasets.CIFAR10(root = opt.dataroot, train=True,download=True, \n",
    "        transform=transforms.Compose([transforms.Resize(opt.imageSize), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "\n",
    "assert dataset \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = opt.batchSize, shuffle=True)\n",
    "\n",
    "# building generator\n",
    "class Generator(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_embed = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        self.depth=128\n",
    "\n",
    "        def init(input, output, normalize=True): \n",
    "            layers = [nn.Linear(input, output)]\n",
    "            if normalize: \n",
    "                layers.append(nn.BatchNorm1d(output, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers \n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "\n",
    "            *init(opt.latentdim+opt.n_classes, self.depth), \n",
    "            *init(self.depth, self.depth*2), \n",
    "            *init(self.depth*2, self.depth*4), \n",
    "            *init(self.depth*4, self.depth*8),\n",
    "            nn.Linear(self.depth * 8, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "\n",
    "            )\n",
    "\n",
    "    # torchcat needs to combine tensors \n",
    "    def forward(self, noise, labels): \n",
    "        gen_input = torch.cat((self.label_embed(labels), noise), -1)\n",
    "        img = self.generator(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_embed1 = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        self.dropout = 0.4 \n",
    "        self.depth = 512\n",
    "\n",
    "        def init(input, output, normalize=True): \n",
    "            layers = [nn.Linear(input, output)]\n",
    "            if normalize: \n",
    "                layers.append(nn.Dropout(self.dropout))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers \n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            *init(opt.n_classes+int(np.prod(img_shape)), self.depth, normalize=False),\n",
    "            *init(self.depth, self.depth), \n",
    "            *init(self.depth, self.depth),\n",
    "            nn.Linear(self.depth, 1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, img, labels): \n",
    "        imgs = img.view(img.size(0),-1)\n",
    "        inpu = torch.cat((imgs, self.label_embed1(labels)), -1)\n",
    "        validity = self.discriminator(inpu)\n",
    "        return validity \n",
    "\n",
    "\n",
    "# weight initialization\n",
    "def init_weights(m): \n",
    "    if type(m)==nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# Building generator \n",
    "generator = Generator()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=opt.lrate, betas=(opt.beta, opt.beta1))\n",
    "\n",
    "# Building discriminator  \n",
    "discriminator = Discriminator()\n",
    "discriminator.apply(init_weights)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=opt.lrate, betas=(opt.beta, opt.beta1))\n",
    "\n",
    "# Loss functions \n",
    "a_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Labels \n",
    "real_label = 0.9\n",
    "fake_label = 0.0\n",
    "\n",
    "FT = torch.LongTensor\n",
    "FT_a = torch.FloatTensor\n",
    "\n",
    "if cuda: \n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    a_loss.cuda()\n",
    "FT = torch.cuda.LongTensor\n",
    "FT_a = torch.cuda.FloatTensor\n",
    "\n",
    "# training \n",
    "for epoch in range(opt.epoch): \n",
    "    for i, (imgs, labels) in enumerate(dataloader): \n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # convert img, labels into proper form \n",
    "        imgs = Variable(imgs.type(FT_a))\n",
    "        labels = Variable(labels.type(FT))\n",
    "\n",
    "        # creating real and fake tensors of labels \n",
    "        reall = Variable(FT_a(batch_size,1).fill_(real_label))\n",
    "        f_label = Variable(FT_a(batch_size,1).fill_(fake_label))\n",
    "\n",
    "        # initializing gradient\n",
    "        gen_optimizer.zero_grad() \n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        #### TRAINING GENERATOR ####\n",
    "        # Feeding generator noise and labels \n",
    "        noise = Variable(FT_a(np.random.normal(0, 1,(batch_size, opt.latentdim))))\n",
    "        gen_labels = Variable(FT(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        gen_imgs = generator(noise, gen_labels)\n",
    "\n",
    "        # Ability for discriminator to discern the real v generated images \n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "\n",
    "        # Generative loss function \n",
    "        g_loss = a_loss(validity, reall)\n",
    "\n",
    "        # Gradients \n",
    "        g_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        #### TRAINING DISCRIMINTOR ####\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # Loss for real images and labels \n",
    "        validity_real = discriminator(imgs, labels)\n",
    "        d_real_loss = a_loss(validity_real, reall)\n",
    "\n",
    "        # Loss for fake images and labels \n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = a_loss(validity_fake, f_label)\n",
    "\n",
    "        # Total discriminator loss \n",
    "        d_loss = 0.5 * (d_fake_loss+d_real_loss)\n",
    "\n",
    "        # calculates discriminator gradients\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "        if i%100 == 0: \n",
    "            vutils.save_image(gen_imgs, '%s/real_samples.png' % opt.output, normalize=True)\n",
    "            fake = generator(noise, gen_labels)\n",
    "            vutils.save_image(fake.detach(), '%s/fake_samples_epoch_%03d.png' % (opt.output, epoch), normalize=True)\n",
    "\n",
    "    print(\"[Epoch: %d/%d]\" \"[D loss: %f]\" \"[G loss: %f]\" % (epoch+1, opt.epoch, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    # checkpoints \n",
    "    torch.save(generator.state_dict(), '%s/generator_epoch_%d.pth' % (opt.output, epoch))\n",
    "    torch.save(discriminator.state_dict(), '%s/generator_epoch_%d.pth' % (opt.output, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
