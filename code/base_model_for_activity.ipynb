{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler,MinMaxScaler\n",
    "X_ppg,X_qual,X_acc,y_rr, y_respiration,y_inspiration,y_expiration,groups,X_respiration,y_activity,y_label = pickle.load(open('../../affect-recognition/data/tabular_data_8.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azim/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "index = np.where((y_activity!=0)&(y_activity!=3)&(y_activity!=6)&(y_activity!=8))[0]\n",
    "X_acc = X_acc[index]\n",
    "y_activity = y_activity[index]\n",
    "groups = groups[index]\n",
    "y_activity = OneHotEncoder().fit_transform(y_activity.reshape(-1,1)).todense()\n",
    "# X_fft = np.fft.fft(X_acc,axis=1)\n",
    "# X_fft_real = X_fft.real\n",
    "# X_fft_imag = X_fft.imag\n",
    "# X_fft = np.concatenate([X_fft_real,X_fft_imag],axis=2)\n",
    "# X_fft = X_fft_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        (None, 256, 3)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 256, 200)          6200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 256, 200)          800       \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 256, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 256, 100)          200100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 256, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 256, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 256, 200)          200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 256, 200)          800       \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 256, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 256, 120)          125280    \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 120)               86880     \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 30)                3630      \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 5)                 155       \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 624,475\n",
      "Trainable params: 623,475\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n",
      "Train on 14732 samples, validate on 6314 samples\n",
      "Epoch 1/400\n",
      "14732/14732 [==============================] - 117s 8ms/step - loss: 1.2519 - accuracy: 0.6519 - val_loss: 1.4799 - val_accuracy: 0.2460\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.24596, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 2/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 1.0236 - accuracy: 0.7209 - val_loss: 1.5566 - val_accuracy: 0.3991\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.24596 to 0.39911, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 3/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.9035 - accuracy: 0.7377 - val_loss: 1.0772 - val_accuracy: 0.6071\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.39911 to 0.60706, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 4/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.8937 - accuracy: 0.7154 - val_loss: 1.5410 - val_accuracy: 0.4937\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.60706\n",
      "Epoch 5/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.8507 - accuracy: 0.7171 - val_loss: 1.4670 - val_accuracy: 0.5158\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.60706\n",
      "Epoch 6/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.7443 - accuracy: 0.7431 - val_loss: 0.9164 - val_accuracy: 0.6878\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.60706 to 0.68784, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 7/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.6669 - accuracy: 0.7548 - val_loss: 0.9201 - val_accuracy: 0.6997\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.68784 to 0.69971, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 8/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.6642 - accuracy: 0.7453 - val_loss: 1.1552 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.69971\n",
      "Epoch 9/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.6289 - accuracy: 0.7602 - val_loss: 0.9454 - val_accuracy: 0.6653\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.69971\n",
      "Epoch 10/400\n",
      "14732/14732 [==============================] - 114s 8ms/step - loss: 0.5951 - accuracy: 0.7634 - val_loss: 0.9005 - val_accuracy: 0.6866\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.69971\n",
      "Epoch 11/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.5738 - accuracy: 0.7695 - val_loss: 1.0944 - val_accuracy: 0.6079\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.69971\n",
      "Epoch 12/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.5645 - accuracy: 0.7683 - val_loss: 0.8556 - val_accuracy: 0.7095\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.69971 to 0.70953, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 13/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.5206 - accuracy: 0.7819 - val_loss: 0.7909 - val_accuracy: 0.7282\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.70953 to 0.72822, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 14/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.5333 - accuracy: 0.7736 - val_loss: 1.1027 - val_accuracy: 0.6090\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.72822\n",
      "Epoch 15/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.5610 - accuracy: 0.7632 - val_loss: 0.9366 - val_accuracy: 0.6601\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.72822\n",
      "Epoch 16/400\n",
      "14732/14732 [==============================] - 114s 8ms/step - loss: 0.5360 - accuracy: 0.7667 - val_loss: 1.0863 - val_accuracy: 0.6478\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.72822\n",
      "Epoch 17/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.5044 - accuracy: 0.7766 - val_loss: 0.6585 - val_accuracy: 0.7746\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.72822 to 0.77463, saving model to ../../affect-recognition/data/models/activity/8v4.h5\n",
      "Epoch 18/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4633 - accuracy: 0.7915 - val_loss: 0.7668 - val_accuracy: 0.7461\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.77463\n",
      "Epoch 19/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4624 - accuracy: 0.7859 - val_loss: 0.8875 - val_accuracy: 0.6992\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.77463\n",
      "Epoch 20/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4605 - accuracy: 0.7894 - val_loss: 0.9476 - val_accuracy: 0.6783\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.77463\n",
      "Epoch 21/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4388 - accuracy: 0.7959 - val_loss: 0.9051 - val_accuracy: 0.6832\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.77463\n",
      "Epoch 22/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4573 - accuracy: 0.7847 - val_loss: 0.8697 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.77463\n",
      "Epoch 23/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4428 - accuracy: 0.7886 - val_loss: 0.8348 - val_accuracy: 0.7279\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.77463\n",
      "Epoch 24/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4115 - accuracy: 0.7964 - val_loss: 0.9005 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.77463\n",
      "Epoch 25/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4267 - accuracy: 0.7943 - val_loss: 0.7848 - val_accuracy: 0.7211\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.77463\n",
      "Epoch 26/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4111 - accuracy: 0.8029 - val_loss: 0.8509 - val_accuracy: 0.7061\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.77463\n",
      "Epoch 27/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4765 - accuracy: 0.7839 - val_loss: 0.8177 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.77463\n",
      "Epoch 28/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4208 - accuracy: 0.8010 - val_loss: 0.7929 - val_accuracy: 0.7162\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.77463\n",
      "Epoch 29/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4426 - accuracy: 0.7885 - val_loss: 0.8129 - val_accuracy: 0.6863\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.77463\n",
      "Epoch 30/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4110 - accuracy: 0.8004 - val_loss: 0.8691 - val_accuracy: 0.6703\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.77463\n",
      "Epoch 31/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4095 - accuracy: 0.8016 - val_loss: 0.9025 - val_accuracy: 0.6882\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.77463\n",
      "Epoch 32/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4386 - accuracy: 0.7897 - val_loss: 0.7740 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.77463\n",
      "Epoch 33/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4009 - accuracy: 0.8051 - val_loss: 0.7277 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.77463\n",
      "Epoch 34/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3895 - accuracy: 0.8089 - val_loss: 0.7852 - val_accuracy: 0.7140\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.77463\n",
      "Epoch 35/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3839 - accuracy: 0.8152 - val_loss: 0.8465 - val_accuracy: 0.6425\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.77463\n",
      "Epoch 36/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3982 - accuracy: 0.8133 - val_loss: 0.8310 - val_accuracy: 0.6774\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.77463\n",
      "Epoch 37/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3796 - accuracy: 0.8157 - val_loss: 0.8893 - val_accuracy: 0.6584\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.77463\n",
      "Epoch 38/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3651 - accuracy: 0.8241 - val_loss: 0.7829 - val_accuracy: 0.7034\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.77463\n",
      "Epoch 39/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3544 - accuracy: 0.8258 - val_loss: 0.8783 - val_accuracy: 0.6904\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.77463\n",
      "Epoch 40/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3418 - accuracy: 0.8334 - val_loss: 0.8356 - val_accuracy: 0.6234\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.77463\n",
      "Epoch 41/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.3760 - accuracy: 0.8223 - val_loss: 1.0365 - val_accuracy: 0.6158\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.77463\n",
      "Epoch 42/400\n",
      "14732/14732 [==============================] - 115s 8ms/step - loss: 0.4002 - accuracy: 0.8124 - val_loss: 1.9242 - val_accuracy: 0.4328\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.77463\n",
      "Epoch 43/400\n",
      "14732/14732 [==============================] - 116s 8ms/step - loss: 0.3801 - accuracy: 0.8171 - val_loss: 0.8413 - val_accuracy: 0.6951\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.77463\n",
      "Epoch 44/400\n",
      "14732/14732 [==============================] - 116s 8ms/step - loss: 0.3712 - accuracy: 0.8211 - val_loss: 0.8276 - val_accuracy: 0.6932\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.77463\n",
      "Epoch 45/400\n",
      "14732/14732 [==============================] - 116s 8ms/step - loss: 0.3723 - accuracy: 0.8211 - val_loss: 0.9273 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.77463\n",
      "Epoch 46/400\n",
      "14732/14732 [==============================] - 116s 8ms/step - loss: 0.3460 - accuracy: 0.8365 - val_loss: 0.8286 - val_accuracy: 0.7265\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.77463\n",
      "Epoch 47/400\n",
      "14732/14732 [==============================] - 116s 8ms/step - loss: 0.3531 - accuracy: 0.8371 - val_loss: 0.7908 - val_accuracy: 0.7179\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.77463\n",
      "Epoch 00047: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector,Bidirectional,Multiply,multiply,Permute\n",
    "from keras.layers import TimeDistributed,Dense,Flatten,Reshape,Lambda,Activation,GRU,Conv1D,MaxPool1D,Dropout,BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras import metrics,losses\n",
    "import tensorflow as tf\n",
    "# import tensorflow_probability as tf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def get_base_CNN(timesteps_ppg=256,input_dim_ppg=3,first_dim=60,\n",
    "                 optimizer='adam',loss='kullback_leibler_divergence',output_dim=5,\n",
    "                 metrics = ['accuracy']):\n",
    "    inputs = Input(shape=(timesteps_ppg, input_dim_ppg))\n",
    "    fe = Conv1D(200,10, padding='same',activation='linear')(inputs)\n",
    "#     fe = MaxPool1D(2)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = Dropout(.1)(fe)\n",
    "    fe = Conv1D(100,10, padding='same',activation='tanh')(fe)\n",
    "#     fe = MaxPool1D(2)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = Dropout(.1)(fe)\n",
    "    fe = Conv1D(200,10, padding='same',activation='tanh')(fe)\n",
    "#     fe = MaxPool1D(4)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = Dropout(.1)(fe)\n",
    "    fe = Bidirectional(LSTM(60,return_sequences=True,activation='tanh'))(fe)\n",
    "    fe = Bidirectional(LSTM(60,return_sequences=False,activation='tanh'))(fe)\n",
    "#     fe = TimeDistributed(Dense(10))(fe)\n",
    "#     flattened = Flatten()(fe)\n",
    "    output = Dense(30,activation='tanh')(fe)\n",
    "    output = Dense(output_dim,activation='sigmoid')(output)\n",
    "    output = Dense(output_dim,activation='softmax')(output)\n",
    "    sequence_autoencoder = Model(inputs=[inputs], outputs=[output])\n",
    "    sequence_autoencoder.compile(optimizer=optimizer,loss=loss,metrics=[metrics])\n",
    "    return sequence_autoencoder\n",
    "\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "indexes = list(gkf.split(X_acc,groups=groups))\n",
    "results = []\n",
    "for i,ii in enumerate(indexes):\n",
    "    train,test = ii\n",
    "    X_acc_train,X_acc_test = X_acc[train],X_acc[test]\n",
    "    y_activity_train,y_activity_test = y_activity[train],y_activity[test]\n",
    "    y_activity_train.shape\n",
    "    sequence_autoencoder = get_base_CNN()\n",
    "    sequence_autoencoder.summary()\n",
    "    filepath = '../../affect-recognition/data/models/activity/'+str(groups[test][0])+'v4.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,save_weights_only=False, mode='max')\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,patience=30)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    history = sequence_autoencoder.fit(X_acc_train, y_activity_train,\n",
    "                    epochs=400,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    validation_split=.3,callbacks=callbacks_list)\n",
    "    %matplotlib notebook\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from keras.models import load_model\n",
    "    sequence_autoencoder.load_weights(filepath)\n",
    "    y_activity_pred = sequence_autoencoder.predict(X_acc_test)\n",
    "    results.append([i,y_activity_test,y_activity_pred])\n",
    "    print(confusion_matrix(np.int64(np.argmax(y_activity_test,axis=1)),np.int64(np.argmax(y_activity_pred,axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.h5 4 class model\n",
    "#0v1.h5 accurate 4 class model\n",
    "#0v2.h5 5 class model\n",
    "#0v3.h5 5 class more accurate model\n",
    "#0.v4 5 class LSTM model\n",
    "\n",
    "pickle.dump(results,open('../../affect-recognition/data/activity_results4.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = np.zeros((5,5))\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "scores = []\n",
    "all_data = []\n",
    "for a in results:\n",
    "    i,y_true,y_pred = a\n",
    "    y_true = np.array(np.int64(np.argmax(y_true,axis=1))).reshape(-1)\n",
    "    y_pred = np.array(np.int64(np.argmax(y_pred,axis=1))).reshape(-1)\n",
    "    y_true_all.extend(list(y_true))\n",
    "    y_pred_all.extend(list(y_pred))\n",
    "    scores.append(np.array([f1_score(y_true,y_pred,average='macro'),\n",
    "                            balanced_accuracy_score(y_true,y_pred),\n",
    "                            accuracy_score(y_true,y_pred)]))\n",
    "    all_data.append([scores[-1][0],'Macro F1'])\n",
    "    all_data.append([scores[-1][1],'Balanced Accuracy'])\n",
    "    all_data.append([scores[-1][2],'Accuracy'])\n",
    "    \n",
    "    \n",
    "#     if confusion_matrix(y_true,y_pred).shape[0]!=5:\n",
    "#         continue\n",
    "#     conf+=confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(all_data,columns=['Value',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(x='',y='Value',data=df)\n",
    "plt.xticks(rotation=5)\n",
    "plt.savefig('../../affect-recognition/data/pics/boxplot.pdf',dps=1e6)\n",
    "plt.show()\n",
    "np.median(scores,axis=0),scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_true_all,y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = conf/np.sum(conf,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(y_true).reshape(-1)\n",
    "from sklearn.metrics import accuracy_score,f1_score,balanced_accuracy_score\n",
    "f1_score(y_true_all,y_pred_all,average='macro'),balanced_accuracy_score(y_true_all,y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,7))\n",
    "# tmp = np.int64(confusion_matrix(y_test_all,y_pred_all))//98\n",
    "sns.heatmap(conf,annot=True,fmt='.2f',annot_kws={\"fontsize\":22},cmap=None,vmin=0,vmax=1,cbar=False)\n",
    "plt.xticks([.5,1.5,2.5,3.5,4.5],['Sitting','Stairs','Cycling','Driving','Walking'],fontsize=22)\n",
    "plt.yticks([.5,1.5,2.5,3.5,4.5],['Sitting','Stairs','Cycling','Driving','Walking'],rotation=0,fontsize=22)\n",
    "plt.savefig('../../affect-recognition/data/pics/confusion_matrix.pdf',dps=1e6)\n",
    "# plt.yticks([0.5,1.5],['LAB','FIELD'])\n",
    "# plt.title('Average Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_GRU(timesteps_ppg=256,input_dim_ppg=3,first_dim=60,\n",
    "                 optimizer='adam',loss='binary_crossentropy',output_dim=9,\n",
    "                 metrics = ['accuracy']):\n",
    "    inputs = Input(shape=(timesteps_ppg, input_dim_ppg))\n",
    "    encoded = Bidirectional(GRU(first_dim,return_sequences=True,activation='tanh',go_backwards=True))(inputs)\n",
    "    dense_layer = TimeDistributed(Dense(1,activation='tanh'))(encoded)\n",
    "    flattened = Flatten()(dense_layer)\n",
    "    output = Dense(30,activation='relu')(flattened)\n",
    "    output = Dense(output_dim,activation='sigmoid')(output)\n",
    "    sequence_autoencoder = Model(inputs=[inputs], outputs=[output])\n",
    "    sequence_autoencoder.compile(optimizer=optimizer,loss=loss,metrics=[metrics])\n",
    "    return sequence_autoencoder\n",
    "\n",
    "\n",
    "def get_base_CNN(timesteps_ppg=256,input_dim_ppg=3,first_dim=60,\n",
    "                 optimizer='adam',loss='kullback_leibler_divergence',output_dim=4,\n",
    "                 metrics = ['accuracy']):\n",
    "    inputs = Input(shape=(timesteps_ppg, input_dim_ppg))\n",
    "    fe = Conv1D(100,4, padding='same',activation='tanh')(inputs)\n",
    "    fe = MaxPool1D(2)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = Dropout(.2)(fe)\n",
    "    fe = Conv1D(100,4, padding='same',activation='tanh')(fe)\n",
    "    fe = MaxPool1D(4)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = Dropout(.2)(fe)\n",
    "    fe = Conv1D(50,4, padding='same',activation='tanh')(fe)\n",
    "    fe = MaxPool1D(8)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = Dropout(.2)(fe)\n",
    "    flattened = Flatten()(fe)\n",
    "    output = Dense(30,activation='relu')(flattened)\n",
    "    output = Dense(output_dim,activation='sigmoid')(output)\n",
    "    sequence_autoencoder = Model(inputs=[inputs], outputs=[output])\n",
    "    sequence_autoencoder.compile(optimizer=optimizer,loss=loss,metrics=[metrics])\n",
    "    return sequence_autoencoder\n",
    "\n",
    "# timesteps_ppg = 256\n",
    "# input_dim_ppg = 3\n",
    "# sequence_autoencoder = get_base_autoencoder(timesteps_ppg=timesteps_ppg,input_dim_ppg=input_dim_ppg,\n",
    "#                                     first_dim=60,optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(X_acc_test[20])\n",
    "# plt.plot(X_ppg_pred[330],'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rr_pred = sequence_autoencoder.predict(X_ppg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_rr_pred)\n",
    "plt.plot(y_rr_test,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = np.abs(y_rr_pred-y_rr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_powers = [np.mean(a[:,-1]) for a in X_qual_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(relative_powers,differences)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
